---
title: "Blog"
listing:
  contents: posts
  sort: "date desc"
  type: default
  categories: true
  sort-ui: false
  filter-ui: false
page-layout: full
title-block-banner: true
format: 
  html:
    code-fold: true
    code-tools: true
    code-line-numbers: true
execute:
  echo: false
  include: false
number-sections: true
toc: true
toc-depth: 3
toc-title: Contents

theme: simplex

repo: calebjpicker/AFI
---

# Introduction to My AFI Project

by [Caleb J. Picker](https://calebjpicker.quarto.pub/ "Caleb J. Picker's Curriculum Vitae") January 18, 2023, created using Quarto in RStudio

[Sing the Sorrow](https://en.wikipedia.org/wiki/Sing_the_Sorrow) is my favorite album of all time. I've listened to it 1000s of times over the last 20 years since its release on March 11, 2003. If you're like me and you love AFI as much as I do, then you likely also love the rest of their discography. And if you're really like me, then you also love data science, research, and statistics.

As the [20th Anniversary of the Sing the Sorrow](https://www.youtube.com/watch?v=k6x3kf5rvak&t=4s) concert looms on the horizon, I was inspired to combine both of my passions to analyze the content of AFI's lyrics from their major album releases (and to upskill my data science skillset in the realm of natural language processing).

So let's see what modern statistics can reveal about the meanings of AFI's lyrics from their major album releases!

To preview, I present the most frequently used words overall and by album. Then, I perform a latent semantic analysis to help uncover how co-occurrences of words relate across their discography. It's pretty cool what this can reveal, and I can't wait to show you. This analysis involves a word cloud, the hidden meanings of words and how they're used in different contexts, and I select certain words that interest me and project them onto the semantic space generated by all the lyrics. For example, is the word "star" more closely related to "born" or "death"?

# AFI's Most Frequently Used Words

In this section, I calculate the most frequently used words and then I group them and show the top 10 most frequently used words by major album release.

To accomplish this, I used the Lyrics Genius API wrapper in the `geniusr`package download AFI's entire catalog of songs (following <https://www.r-bloggers.com/2021/01/scraping-analysing-and-visualising-lyrics-in-r/>).

```{r}
#| echo: false

# Set working directory
dir = 'C:/Users/caleb/OneDrive/Caleb/Personal/Financials/Curriculum Vitae/Portfolio'
setwd(dir)
```

```{r}
#| echo: false
#| cache: false
library(jsonlite)
library(tidyverse)
library(geniusr)
library(tidytext)
library(ggplot2)
library(quanteda)
# library(spacyr)
# library(openNLP)
# library(rJava)
library(glue)
library(gplots)
library(RColorBrewer)
library(ComplexHeatmap)
library(wordcloud)
library(rjson)

```

```{r}
#| echo: false
#| output: false
#| cache: true
library(geniusr)# find artist ID
# genius token API
gen_token="CqypOkltZXuxwnwuEvLm6OQB_uonxqSJ-w-rWIBamPSAg0sJdEia2g1iCZSNwWhD" # NEED TO HIDE THIS SOMEHOW
# Find the artist ID.
search_artist("AFI",access_token=gen_token) #36520
# Save Artist ID as variable.
AFI_id<-36520
# Get all songs based on Artist ID variable.
songs<-get_artist_songs_df(AFI_id,access_token = gen_token)
# Copy songs as new dataframe (to preserve original import)
songs_df<-songs
# Get song ids
```

```{r}
#| freeze: true
#| cache: true
ids <- c(as.character(songs_df$song_id))
# remove songs that are repeats or live versions or not on a main album
# create list of songs by song id to remove
songs_to_rm<- c(1795101,	# 100 Words
1955285,	# 17 Crimes (LA Riots Remix)
775819,	# 3.5
1984466,	# A Single Second - Live From Long Beach Arena
774610,	# A Winter's Tale
4046936,	# Back into the Sun
794933,	# Born In The US Of A
4046935,	# Break Angels
2389822,	# Breathing Towers to Heaven
794998,	# But Home Is Nowhere (Demo Version)
1795120,	# But Home Is Nowhere (Demo Version) [from "the Leaving Song Pt.Ii" (Part 2) Single]chicken Song
775075,	# Carcinogen Crush
7250231,	# Caught
1707354,	# Demonomania
1178936,	# Don't Change
795088,	# Dream Of Waking
1966520,	# Endlessly, She Said - Live From Long Beach Arena
785517,	# Ether
2117778,	# Fainting Spells
1323191,	# Fall Children
785146,	# Fallen Like the Sky
4045137,	# Get Dark
1946768,	# Girl's Not Grey - Live From Long Beach Arena
2009937,	# God Called In Sick Today - Live From Long Beach Arena
782859,	# Halloween
785634,	# Head Like a Hole
794623,	# Hearts Frozen Solid, Thawed Once More by the Spring of Rage, Despair and Hopelessness
2160131,	# I Hope You Suffer
2323968,	# Initation
2118067,	# Intro/Prelude 12/21
794676,	# I Wanna Mohawk
970901,	# Jack the Ripper
779434,	# Key Lime Pie
1955497,	# Kill Caustic - Live From Long Beach Arena
775594,	# Kung-Fu Devil
1772410,	# Love Is A Many Splendored Thing
784805,	# Love Is a Many Splendored Thing [vinyl only] (Ft.Â BrettÂ Reed & TimÂ Timebomb)
1979083,	# Love Like Winter - Live From Long Beach Arena
484287,	# Lower It
794800,	# Lower Your Head And Take It In The Body
784844,	# Malleus Malleficarum
775186,	# Man in a Suitcase
785056,	# Mini Trucks Suck
4663303,	# MissedCallz [interlude]
1945711,	# Miss Murder - Live From Long Beach Arena
4965495,	# Miss Murder (UK Radio Edit)
774660,	# My Michelle
775479,	# Now the World
775962,	# On the Arrow
779513,	# Over Exposure
1855407,	# Paper Airplanes
785140,	# Paper Airplanes (Makeshift Wings) (Demo Version)
784382,	# Porphyria
1994239,	# Prelude 12/21 - Live From Long Beach Arena
795301,	# Rabbits Are Roadkill On Route 37
774434,	# Rabbits Are Roadkill on Rt. 37
775770,	# Red Hat
774620,	# Reiver's Music
775217,	# Rolling Balls
784209,	# Self Pity
1964228,	# Silver And Cold - Live From Long Beach Arena
1981611,	# Summer Shudder - Live From Long Beach Arena
795228,	# Synesthesia
1795242,	# Synesthesia (Demo Version)
785775,	# The Boy Who Destroyed the World
794969,	# The Chicken Song
2000230,	# The Days of the Phoenix - Live From Long Beach Arena
785608,	# The Great Disappointment (Demo Version)
742667,	# The Hanging Garden
776173,	# The Leaving Song (Demo Version)
784751,	# The Leaving Song Part II
1986158,	# The Leaving Song Pt. II - Live From Long Beach Arena
1935590,	# The Missing Frame - Live From Long Beach Arena
4046937,	# The Missing Man
250706,	# The Spoken Word
2835040,	# The View From Here
1748926,	# This Celluloid Dream (Demo Version)
1940476,	# This Time Imperfect - Live From Long Beach Arena
2389819,	# Too Late for Gods
785951,	# Totalimmortal
785386,	# Transference
4046934,	# Trash Bat
967979,	# Values Here
775159,	# We Bite
2389817,	# Weâ€™ve Got the Knife
775743,	# Whatever I Do
2128444,	# Where We Used to Play
795523,	# Who Knew?
784581,	# Who Said You Could Touch Me?
775274,	# Who said you could touch me? [vinyl only]
1059885,	# YÃ¼rf Rendenmein
1795153	# Ziggy Stardust

)
songs_to_rm_names<-c(
"100 Words",
"17 Crimes (LA Riots Remix)",
"3.5",
"A Single Second - Live From Long Beach Arena",
"A Winter's Tale",
"Back into the Sun",
"Born In The US Of A",
"Break Angels",
"Breathing Towers to Heaven",
"But Home Is Nowhere (Demo Version)",
"But Home Is Nowhere (Demo Version) [from the Leaving Song Pt.Ii (Part 2) Single]chicken Song",
"Carcinogen Crush",
"Caught",
"Demonomania",
"Don't Change",
"Dream Of Waking",
"Endlessly, She Said - Live From Long Beach Arena",
"Ether",
"Fainting Spells",
"Fall Children",
"Fallen Like the Sky",
"Get Dark",
"Girl's Not Grey - Live From Long Beach Arena",
"God Called In Sick Today - Live From Long Beach Arena",
"Halloween",
"Head Like a Hole",
"Hearts Frozen Solid, Thawed Once More by the Spring of Rage, Despair and Hopelessness",
"I Hope You Suffer",
"Initation",
"Intro/Prelude 12/21",
"I Wanna Mohawk",
"Jack the Ripper",
"Key Lime Pie",
"Kill Caustic - Live From Long Beach Arena",
"Kung-Fu Devil",
"Love Is A Many Splendored Thing",
"Love Is a Many Splendored Thing [vinyl only] (Ft.Bretteed & TimTimebomb)",
"Love Like Winter - Live From Long Beach Arena",
"Lower It",
"Lower Your Head And Take It In The Body",
"Malleus Malleficarum",
"Man in a Suitcase",
"Mini Trucks Suck",
"MissedCallz [interlude]",
"Miss Murder - Live From Long Beach Arena",
"Miss Murder (UK Radio Edit)",
"My Michelle",
"Now the World",
"On the Arrow",
"Over Exposure",
"Paper Airplanes",
"Paper Airplanes (Makeshift Wings) (Demo Version)",
"Porphyria",
"Prelude 12/21 - Live From Long Beach Arena",
"Rabbits Are Roadkill On Route 37",
"Rabbits Are Roadkill on Rt. 37",
"Red Hat",
"Reiver's Music",
"Rolling Balls",
"Self Pity",
"Silver And Cold - Live From Long Beach Arena",
"Summer Shudder - Live From Long Beach Arena",
"Synesthesia",
"Synesthesia (Demo Version)",
"The Boy Who Destroyed the World",
"The Chicken Song",
"The Days of the Phoenix - Live From Long Beach Arena",
"The Great Disappointment (Demo Version)",
"The Hanging Garden",
"The Leaving Song (Demo Version)",
"The Leaving Song Part II",
"The Leaving Song Pt. II - Live From Long Beach Arena",
"The Missing Frame - Live From Long Beach Arena",
"The Missing Man",
"The Spoken Word",
"The View From Here",
"This Celluloid Dream (Demo Version)",
"This Time Imperfect - Live From Long Beach Arena",
"Too Late for Gods",
"Totalimmortal",
"Transference",
"Trash Bat",
"Values Here",
"We Bite",
"Weâ€™ve Got the Knife",
"Whatever I Do",
"Where We Used to Play",
"Who Knew?",
"Who Said You Could Touch Me?",
"Who said you could touch me? [vinyl only]",
"YÃ¼rf Rendenmein",
"Ziggy Stardust"
)

# remove songs from songs_to_rm list
songs_df<-songs_df[!(songs_df$song_id %in% songs_to_rm),]


# get all song ids (should now exclude songs on songs_to_rm)
ids <- as.character(songs_df$song_id)
```

```{r}
#| echo: false
#| freeze: true
#| include: false
#| cache: true
# Create empty dataframe to house the lyrics
allLyrics<-data.frame()
# add lyrics to df

while (length(ids) > 0) {
  for (id in ids) {
    tryCatch({
      allLyrics <- rbind(get_lyrics_id(id,access_token = gen_token), allLyrics)
      successful <- unique(allLyrics$song_id)
      ids <- ids[!ids %in% successful]
      print(paste("done - ", id))
      print(paste("New length is ", length(ids)))
    }, error = function(e){})
  }
}

# create copy of all Lyrics
allLyrics_orig<-allLyrics

# create dtaframe with lyrics for all songs and add empty column for album info
allIds <- data.frame(song_id = unique(allLyrics$song_id))
allIds$album <- ""
```

```{r}
#| echo: false
#| freeze: true
#| include: false
#| cache: true
# get all album info and join it to the allLyrics df
for (song in allIds$song_id) {
  allIds[match(song,allIds$song_id),2] <- get_song_df(song,access_token = gen_token)[12]
  print(allIds[match(song,allIds$song_id),])
}

# add album info for Yurf Rendenmein
allIds[(allIds$song_id==1817423),2]<-"Very Proud of Ya"

```

```{r}
#| freeze: true
#| include: false
#| cache: true
# remove repeated lines within allLyrics (e.g., repeated choruses)
allLyrics_unique<-allLyrics %>%
  distinct(allLyrics,line,keep.all=TRUE)

# keep only major release albums
# allIds<-allIds[(allIds$album %in% album_major_release),] 
# join allIds dataframe with allLyrics_unique dataframe
allLyrics_unique <- full_join(allIds, allLyrics_unique)
# keep only major release albums (in case full join added them back in, but they shouldn't have)
# allLyrics_unique<-allLyrics_unique[(allLyrics_unique$album %in% album_major_release),]# keep only major release albums

allLyrics2 <- full_join(allLyrics_unique, allIds)
```

## Songs Removed from Analysis

Then I filtered to only those songs in their major album release. This means I removed almost 100 songs from this analysis, as AFI's catalog contains about 240 songs. See below for list of removed songs.

```{r}
#| echo: false
#| include: true
#| code-fold: true

DT::datatable(as.matrix(songs_to_rm_names),
              caption="This table shows the songs removed from analysis because they were not part of the major album release.",
              colnames = "Removed Song")

```

For transparency, I next show the most frequent words with and without stop words.

```{r}
#| freeze: false

library(tidyverse)
allLyricsTokenised <- allLyrics2 %>%
  #word (changed to ngram) is the new column, line the column to retrieve the information from
  unnest_tokens(word, line,token="words")

# allLyricsTokenised<-distinct(allLyricsTokenised,word,song,keep.all=TRUE)
dim(allLyricsTokenised)
```

## Word Frequency {#word-frequency-with-and-without-stop-words}

To begin pre-processing the lyrics, I first removed lines of words that repeated (e.g., choruses), and then I removed stop words (e.g., "I", "you", "the", "a"). Therefore, this section shows word frequency both with stop words (Table 1-left) and without stop words (Table 2-right). (**Note**: Each table is [interactive]{.underline} and [searchable]{.underline}!)

As an aside, an analysis with stop words would definitely be interesting. Davey uses pronouns and direct objects in unique ways, and that could be an entire analysis of its own: The stop words could be contextualized, for example, using ngrams.

Without further ado, let's reveal AFI's most frequently used words! Looking at Table 2 (right), it seems like the top 5 words are `feel` (75), `love` (70), `eyes` (60), `time` (55), and `life` (51), and `heart` (49).

```{r}
#| echo: false
#| include: true
#| output: true
#| freeze: false
#| layout-ncol: 2
#| column: page
#| tbl-cap: "Word frequency across AFI's major album releases."
#| tbl-subcap: 
#|    - "Stop words present"
#|    - "Stop words removed"



library(jsonlite)

# Count each word
allLyricscount<-allLyricsTokenised %>%
  count(word, sort = TRUE)

DT::datatable(allLyricscount,
              caption = "Table 1: Word frequency across all of AFI's major album releases (with stop-words).",
              colnames=c("Word","Frequency")
  #             options = list(
  # initComplete = JS(
  #   "function(settings, json) {",
  #   "$(this.api().table().header()).css({'background-color': '#92141c', 'color': '#767975'});",
  #   "}")
                )
  # library(rjson)
library(jsonlite)
# library(rjson)

# Remove stopwords
tidyLyrics <- allLyricsTokenised %>%
  anti_join(stop_words)

tidyLyricscount<-tidyLyrics %>%
  count(word, sort = TRUE)

DT::datatable(tidyLyricscount,
              caption = "Table 2: Word frequency across all of AFI's major album releases (with stop-words removed).",
              colnames=c("Word","Frequency")
  #             options = list(
  # initComplete = JS(
  #   "function(settings, json) {",
  #   "$(this.api().table().header()).css({'background-color': '#92141c', 'color': '#767975'});",
  #   "}")
                 )
  

# tidyLyrics


```

## Top 10 Most Frequently Used Words (by Album)

In this section, I grouped the lyrics by major album release and sorted them (following <https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets>). This allowed me to create a plot of AFI's top 10 most frequently used words faceted by major album release. I tried my best to theme up the image and use Sing the Sorrow colors. I also tried to select colors from each album to represent the data as bars.

Following on the theme, let's look at Sing the Sorrow (the blood-red album in the second row, second column). The top 6 words are `grey`, `tonight`, `dance`, `step`, `lay`, `inside`, and `heart`. Grey likely comes from This Celluloid Dream ("All the colors (all grey) upon leaving (all grey) all will turn to grey.")

Feel free to look at the rest of the albums! It's pretty interesting! Let me know what else you discover!

```{r}
#| echo: false
#| freeze: true
#| output: false
#| include: false
topFew <- tidyLyrics %>%
  group_by(album, word) %>%
  mutate(n = row_number()) %>%
  ungroup()


# Take only top 10 max for each word by album
topFew <- topFew %>%
  group_by(album, word) %>%
  summarise(n = max(n))%>%
  mutate(total=sum(n)) %>%
  top_n(10,n) %>%
  ungroup() %>%
 arrange(album,n) %>%
  # 3. add order column of row numbers
  mutate(order=row_number())

topFew <- topFew %>% 
  mutate(total = sum(n)) %>%
  group_by(word) %>%
  # top_n(10,abs(n)) %>%
  # filter(total>=40) %>%
  # 1. remove grouping
  ungroup() %>%
  # 2. Arrange by
  # i. facet group
  # ii. bar height
  arrange(album,total) %>%
  # 3. add order column of row numbers
  mutate(order=row_number())

```

```{r}
#| echo: false

# Create a facet for each album and list the most frequented words for that album
# https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets

library(plotly) # package to make plot interactive
library(jpeg)
library(grid)
# afi_sts_bg<-jpeg::readJPEG("afi-sing-sorrow.jpg")

# Prepare only albums for major release
album_major_release<-c("Answer That and Stay Fashionable",
                     "Very Proud of Ya",
                     "Shut Your Mouth and Open Your Eyes",
                     "Black Sails in the Sunset",
                     "The Art of Drowning",
                     "Sing the Sorrow",
                     "DECEMBERUNDERGROUND",
                     "Crash Love",
                     "Burials",
                     "AFI (The Blood Album)",
                     "Bodies")
  
  # colours for each album

  
  
  albumCol <- c("#c9a95a", # ATASF      
              "#a0480f", # VPYa    
              "#dc4d33", #SYMAOYE    
              "#e86722", # Black Sails in the Sunset  
              "#9f62ad", # The Art of Drowning     
              "#92141c", # Sing the Sorrow    
              "#d6dee5", # DU
              "#f3c937", # Crash Love    
              "#7c7c7c", # Burials    
              "#3b0404", # AFI (The Blood Album)    
              "#595957") # Bodies
names(albumCol) <- album_major_release

# This ensures bars are stacked in order of release date
topFew$album <- factor(topFew$album, levels = album_major_release)


wordsPlot <- ggplot(topFew) +
  
  geom_bar(aes(x = order, 
               y = n,
               fill = as.factor(album)),
           colour = "#141517",
           stat = "identity",
           show.legend = FALSE) + # removes legend
  
  coord_flip() +
  
  facet_wrap(~ album ,scales="free") +
  
  labs(title = "AFI's Top 10 most frequently used words by album",
       
       caption = "Source: genius.com | by @CalebPicker",
       y = "Frequency",
       x = "Word",
       fill = "Album") +
  
  # annotation_custom(rasterGrob(afi_sts_bg,
  #                              width = unit(1,"npc"),
  #                              height = unit(1,"npc")),
  #                              -Inf,Inf,-Inf,Inf) +
  
  scale_fill_manual(values = albumCol) +
  
  # add categories to axis
  scale_x_continuous(
    breaks = topFew$order,
    labels = topFew$word,
    expand = c(0,0)
  )                                   +
  
  theme(title = element_text(face = "bold", size = 16), 
        
        strip.background=element_rect(fill='#141517'),
        strip.text.x = element_text(color="#d51020"), # This impacted teh titles for each facet grid
        strip.text.y = element_text(color="#d51020"),
         strip.text = element_text(size=16,color="#d51020"),
        
        panel.border = element_rect(colour = "#141517", fill=NA, size=1),
        panel.background = element_rect(colour = "#767975", fill = "white"),
        panel.grid.major.x = element_line(colour="#d51020",size = 1, linetype = 4),
        
        axis.title = element_text(face = "bold",size = 16, colour = "#d51020"),
        axis.ticks.length = unit(5, units = "pt"),
        
        legend.position="none",
        # legend.background = element_rect(color='#141517', fill='#141517'),
        # legend.title = element_text(color='#d51020'),
        # legend.position = "top",
        # legend.key.size = unit(12,"pt"),
        # legend.box.spacing = unit(3,"pt"),
        # legend.text = element_text(size = 12,color='#d51020'),
        
        axis.text.y = element_text(size = 12,face="bold"),
        axis.text.x = element_text(size = 12,face="bold"),
        
        plot.background=element_rect(fill='#141517',color='#141517'),
        plot.title=element_text(color='#d51020'),
        plot.subtitle=element_text(color='#d51020'),
        
        )


```

```{r}
#| echo: false
#| include: true
#| fig-height: 12
#| fig-width: 14
# plotly::ggplotly(wordsPlot,show.legend=FALSE)
wordsPlot

# ggsave(filename = "/Albums/AFIwordsbyalbum.png", plot = wordsPlot, width = 30, height = 24, units = "cm",
# type = "cairo")

```

# Latent Semantic Analysis

In this section, I took a slightly different approach to calculating frequency. Whereas in the previous sections, I calculated raw frequency, in this and the following sections I wanted to capture the reasons why Davey used the words he did. To start the process, therefore, I followed [Gefen, Endicott, Fresneda, Miller, and Larsen (2017)](https://aisel.aisnet.org/cais/vol41/iss1/21/ "Latent Semantic Analysis"){#lsa} process for a latent semantic analysis. Briefly, a latent semantic analysis analyzes the similarities among word usage to discover how the same word, for example, can be used to mean different things. Does Davey use the word "star" to more closely resemble "born" or "death"?

## Technical Details

If you're not into techincal details, feel free to skip this section and go straight to [Cosine Similarity (Getting More Interesting!)](#cosine-similarity-getting-more-interesting). Gefen et al.'s (2017) process allowed me to locally and globally weight the raw frequencies. The local weighting algorithm weights words more heavily that appear more often *within* a song (presumably because they're more important); the global weighting algorithm weights words less heavily if words appear more often *across* all songs (presumably because they're less important, like the word "the"). These weightings were then multiplied together.

To start the process, I split up all the songs from the major album releases into separate text files. Then I counted the raw frequencies like before. Then I locally and globally weighted the raw frequencies. The rest of the analyses for this post rely on these weighted frequencies.

```{r}
#| echo: false
#| freeze: true
#| cache: true
library(LSAfun)
library(lsa)

# Import list of default stop words in English
data(stopwords_en)
# stopwords_en

# Create the TDM from all the Words

tidyLyrics_select<-tidyLyrics %>%
  select(song_name,word) # song name and word
# DT::datatable(tidyLyrics_select) # for troubleshooting
# split the tidyLyrics into multiple dataframes based on song

AFI_songs_df_split<- split(tidyLyrics_select,tidyLyrics_select$song_name)
# change name of Prelude 12/21 list to remove slash '/' both the name of hte list and within the list

names(AFI_songs_df_split)[89]<-"Prelude 12 21"
AFI_songs_df_split[['Prelude 12 21']][1]<-"Prelude 12 21"
```

```{r}
#| echo: false
#| freeze: true
#| cache: true
# # Create function to remove song_name column within each list element
# AFI_songs_df_split<-lapply(AFI_songs_df_split, function(x) x[!(names(x) %in% c("song_name"))])
dir<-"C:/Users/Caleb/OneDrive/Caleb/Personal/Financials/Curriculum Vitae/Blog"
# # Create function to export as txt files
lapply(1:length(AFI_songs_df_split),function(x){
  songname<-as.character(names(AFI_songs_df_split[x]))
  outfile<-paste0(dir,"/AFIsongs/",songname,".txt")
  write.table(AFI_songs_df_split[[x]]["word"],
              file = outfile,
              row.names=FALSE,
              col.names=FALSE, # remove the word "word" from all documents
              quote=FALSE)
})
```

```{r}
#| freeze: false
#| echo: false
#| cache: true
# Next I will follow a more practical example so I can learn how to interpret the output better, besides a wordcloud
# Also, make sure that all from major albumreleases are included and others are excluded (didn't seem to be the case for some reason)
# Load required code libraries
library(cluster)
library(tm)
library(LSAfun)
library(lsa)
dir = 'C:/Users/caleb/OneDrive/Caleb/Personal/Financials/Curriculum Vitae/Blog'
setwd(dir)
# Create text matrix using tm's DirSource.

# Create corpus in memory 
# There's some mistyped code that doesn't work in this paper
# raw_corpus<-VCorpus(source_dir,readerControl-list(language='en'))
#I'll copy and paste from previous example to create a raw corpus
# Establish source directory for all text files. (one .txt file = 1 song)
source_dir<-paste0(dir,"/AFIsongs/")

data(stopwords_en)

# Create TDM (text document matrix) with each txt file equal to one document
TDM<-textmatrix(source_dir,stopwords=c(stopwords_en),stemming=TRUE,
                 removeNumber=F,minGlobFreq=1)
# TDM #  view TDM
TDM_summary<-summary.textmatrix(TDM)
TDM_summary_terms<-t(TDM_summary["vocabulary"])
TDM_summary_docs<-t(TDM_summary["documents"])

# Before running the SVM, create a weighted matrix TDM2.
# TDM2 is term frequency x inverse document frequency (this is a standard method)
# lw = local weighting, more importance to terms that appear more times within a single document
# gw = global weighting, less importance to w9rds that appear in more document (same reasoning as removing stop words)
TDM2 <-lw_tf(TDM)*gw_idf(TDM)
# TDM2

```

Next, I imported all the .txt files (one .txt file is one song) to make a collection of documents called a corpus. I set the minimum global frequency to 1. There are `r TDM_summary_terms` vocabulary words and `r TDM_summary_docs` songs. After that, I created several matrices that have song loadings and document loadings (similar to factor analysis). However, for brevity, I will only present the results based on cosine similarity.

```{r}
#| include: false
#| echo: false

library(cluster)
library(tm)
library(LSAfun)
library(lsa)
# Run LSA on wegihted matrix TDM2.
# Number of dimensions chosen by default with dimcalc_share()
# lsa transformed TDM2 into three matrices and placed them all into the miniLSAspace object
# tk = term matrix, dk = document matrix, and sk = singular value matrix
miniLSAspace<-lsa(TDM2,dims=dimcalc_share())
# View matrix by transforming SVD matrix into text matrix
# as.textmatrix(miniLSAspace)
# Doesn't work.  Needs other non-working script.
# findFreqTerms(TDM2)

# these are all unrotated "loadings", like PCA or factor analysis
# This command will show the value-weighted matrix of Terms
# factoring of terms, # factors = 40, so there are 40 factors with loadings for each term
tk2=t(miniLSAspace$sk*t(miniLSAspace$tk))
# round(tk2,2)
#DT::datatable(round(tk2,2),
#              fillContainer=TRUE)

```

```{r}
#| include: false
#| echo: false

# This will show the matrix of documents
# factoring of documents, # factors = 40, so there are 40 factors with loadings for each documents
dk2 = miniLSAspace$dk
# round(dk2,2)
#DT::datatable(round(dk2,2),
#              fillContainer = TRUE)
```

```{r}
#| include: false
#| echo: false
# The sk matrix of singular values connects tk and dk matrices to reproduce the the original TDM2
# Because the sk matrix only has diagonal values, R stores it as a numeric vector
sk2 = miniLSAspace$sk
# round(sk2,2)

```

## Cosine Similarity (Getting More Interesting!) {#cosine-similarity-getting-more-interesting}

Similar to concepts like [correlation](https://en.wikipedia.org/wiki/Correlation), here I present results based on a similarity metric called [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). Cosine similarity ranges between -1 and +1:

-   closer to -1 means the words are more opposite,

-   closer +1 means the words are more similar, and

-   closer to 0 means the words are unrelated.

Note that the diagonal contains values of 1, which means the comparison is between the word/song and itself (similar to a correlation table).

Table 3 shows cosine similarity among all words across all albums. Table 4 shows cosine similarity among all songs. I highly recommend you explore this to your own interests. I'll analyze a small sample from each Table to show you the cool stuff you can glean!

### Related Words

In this interactive table, you can use search to narrow down by rows. However, I'm interested in sorting the values based on a column. In this case, I'll use the word "eye". I click the column header, and I find the bottom 3 words are "awaken", "na" and "remake" with cosine similarity ratings of -.09, -.09, and -.09 respectively. So the use of the word "heart" across all the albums is somewhat dissimilar from the use of the words "awaken", "na" and "remake".

Now let's sort in descending order. I click the column header for "eye" one more time. Aside from the word "eye" itself (cosine similarity value of 1), the top 3 words are "build", "critic", and "els" with cosine similarity values of +.65, +.65, and +.65.

Apparently the use of the word "eye" is more closely related to buliding (a relationship perhaps?), criticism, or they usually refer to someone else's eyes. A deeper analysis would be necessary to understand the context and valence of how these words are used similarly.

```{r}
#| echo: false
#| freeze: false
#| include: true
#| cache: true
myTerms2<-rownames(tk2)
myCosineSpace2<-multicos(myTerms2,tvectors=tk2) # removed breakdown=TRUE arg, and it worked
DT::datatable(round(myCosineSpace2,2),
              options=list(scrollX=TRUE),
              caption="Table 3: Cosine similarity for relationships between individual words.")
```

### Related Songs

Table 4 is similar to Table 3 in that they're both interactive; however, the cosine similarity ratings are calculated by song rather than by word.

Among the major album releases, my all-time favorite album is Sing the Sorrow. It's difficult to narrow down my favorite song because all the songs are so good. At one point in my life, my favorite song was This Celluloid Dream (Death of Seasons at another, This Time Imperfect at another, and so on!).

Similar to what I did for "heart" above, I used the horizontal scroll bar to find "This Celluloid Dream.txt". The bottom 3 are in the negative category, and the top 3 are in the positive category:

-   Negative: "Far Too Near" -.05; "Three Seconds Notice" -.05; "The Face Beneath the Waves" -.04

-   Positive: "Twisted Tongues" +.14; "Strength through Wounding" +.13; "I Am Trying Very Hard to be Here" +.11

Apparently, This Celluloid Dream has some overlap with songs across their discography, and is most closely related to "Twisted Tongues"! This is probably because both songs use the stem 'twist' quite frequently (although, remember, all lines, including choruses, only show up once in my analysis).

Tell me some of the interesting things you discovered in the comment section below!

```{r}
#| echo: false
#| freeze: false
#| include: true
#| cache: true
# STep 2: Analyzing the Semantic Space 
# assumes that miniLSAspace has been created
# Now we will calculate cosine similarity
myDocs<-rownames(dk2)

# round(myCosineSpace2,2)
# DT::datatable(round(myCosineSpace2,2))
# Can create cosine space for documents as well
myCosineSpace3<-multicos(myDocs,tvectors=dk2)
DT::datatable(round(myCosineSpace3,2),
              options=list(scrollX=TRUE,scrollY=TRUE),
              caption="Table 4: Cosine similarity table for relationships between songs.")
```

## Wordcloud {#sec-wordcloud}

Similar to the [Word Frequency (with and without stop-words)](#word-frequency-with-and-without-stop-words) presented earlier, here I present a wordcloud based on local and global weightings.

Take a look! The top 2 words appear to be "feel" and "love". What else caught your "eye"?

```{r}
#| echo: false
#| include: true
# plot wordcloud using lsa results!!!
Term_count<-apply(TDM2,1,sum)
TCT<-t(Term_count)

suppressWarnings(wordcloud(myTerms2,TCT,min.freq=3,random.order=FALSE,color=brewer.pal(8,"Dark2")
          ,scale=c(3.5,.1)))
```

## How Words Relate to Each Other in Latent Semantic Space

Now let's see how these frequent words relate to other words in the semantic space. Here, I created heatmaps of the words and their cosine similarity.

### "feel" Has Four Meanings!

Using the wordcloud to determine the most frequently encountered words, I started with the word **"feel"**.

Values closer to dark green are more similar. Notice also the dendrograms. These dendrograms show how certain words cluster together. At the top of the dendrogram, you can see there is a split signifying there are four clusters of words. That is, `feel` is used to mean four different things. Note that the cosine similarity ratings are darkest green (most similar) for Cluster 1 and Cluster 2:

1.  In Cluster 1, "feel" is related to word stems such as: "wall", "heal", "picture", "strang".

2.  In Cluster 2, "feel" is related to word stems such as: "medic", "poison", "thorn", "ribbon", "gift", "piec", and "perfect"

3.  In Cluster 3, "feel" is related to word stems such as: "ceas", "weak", "hope", and "disappear"

4.  In Cluster 4, "feel" is related to word stems such as: "dark", "worse", and "hurt"

#### My Interpretation of "feel"

"Feel" seems to be most related to confinement in an unfamiliar setting (Cluster 1) and receiving or giving gifts of poisonous pieces of oneself or others (Cluster 2), and to a lesser extent, existential struggles (Cluster 3), and painful darkness (Cluster 4). Of course, my interpretations may not be correct, so I encourage you to analyze this further!

What do you think "feel" means across these different contexts?

```{r}
#| include: true
#| echo: false
#| fig-cap: "Figure 3: Heatmap of the word FEEL and its relation to top 20 words in latent semantic space."
# Show words on a heat map (authors say this is a compelling visualization)


# Extract the closet words to "death" (a list of their distances as a named vector)
words<-neighbors("feel",n=20,tvectors=tk2[,1:ncol(tk2)])
# Extract the actual words, and find the distances in the space
myCosineSpace2<-multicos(names(words),tvectors=tk2[,1:ncol(tk2)])
col<-colorRampPalette(brewer.pal(9,"RdYlGn"))(256)
heatmap.2(myCosineSpace2,col=col)
ht1<-Heatmap(myCosineSpace2,col=col)


```

### "love" Has Three Meanings!

This heatmap shows how "love" is used in three different ways!

1.  In Cluster 1, love is related to word stems such as: "dare", "drop", "appeal", "red"

2.  In Cluster 2, "love" is related to word stems such as: "simpl",,"sweet", "fatal", "lip", "met", "leave", "moment"

3.  In Cluster 3, "love" is related to word stems such as: "blood", "exhale", "trace", "warn", "war", "bit", and "yesterday"

#### My Interpretation of "love"

"Love" seems to more related (darker green) to attraction (Cluster 1) and past bloody battles (Cluster 3) and to a lesser extent, fleeting and fatal encounters (Cluster 2). Of course, my interpretations may be off, so I encourage you to analyze this further!

How do you see "love" being used differently across these contexts?

```{r}
#| include: true
#| echo: false
# Extract the closet words to "birth" (a list of their distances as a named vector)
words<-neighbors("love",n=20,tvectors=tk2[,1:ncol(tk2)])
# Extract the actual words, and find the distances in the space
myCosineSpace2<-multicos(names(words),tvectors=tk2[,1:ncol(tk2)])
col<-colorRampPalette(brewer.pal(9,"RdYlGn"))(256)
heatmap.2(myCosineSpace2,col=col)
ht2<-Heatmap(myCosineSpace2,col=col)



```

### Is "star" More Closely Related to "Born" or "Death"?

In this final section, I was interested in Davey's use of the word star, as featured in songs such as Morningstar, Death of Seasons, and This Celluloid Dream. I thought "star" might signify being born or dying, so I first compare "star" to "born" and "death".

```{r}
#| include: true
# pseudo<-"Does star mean birth or death"
# pseudo<-tolower(pseudo)
# pseudo<-removeWords(pseudo,tm::stopwords('en'))
# # pseudo<-remove_nonletter(pseudo)
# pseudo<-stemDocument(PlainTextDocument(pseudo))
# pseudo<-termFreq(pseudo)
# pseudo<-vapply(pseudo,function(x) log2(x+.00001,numeric(1)))
# pseudo<-mapply(function(x,y) x*y,pseudo,word_entropy[names(pseudo)]) # get this working
# pseudo<-colSums(pseudo*su_mat[names(pseudo),]) # get this working

# Project onto new semantic space (prediction)
print(paste0("Star and born have cosine similarity rating of ", round(costring("star","born" ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Star and death have cosine similarity rating of ", round(costring("star","death" ,tvectors=tk2[,1:ncol(tk2)]),2)))

# # red
# color<-"red"
# 
# print(paste0(color," and love have cosine similarity rating of ", round(costring(color,"love" ,tvectors=tk2[,1:ncol(tk2)]),2)))
# print(paste0(color," and feel have cosine similarity rating of ", round(costring(color,"feel" ,tvectors=tk2[,1:ncol(tk2)]),2)))
# 
# # white
# color<-"white"
# print(paste0(color," and love have cosine similarity rating of ", round(costring(color,"love" ,tvectors=tk2[,1:ncol(tk2)]),2)))
# print(paste0(color," and feel have cosine similarity rating of ", round(costring(color,"feel" ,tvectors=tk2[,1:ncol(tk2)]),2)))
# 
# # black
# color<-"black"
# print(paste0(color," and love have cosine similarity rating of ", round(costring(color,"love" ,tvectors=tk2[,1:ncol(tk2)]),2)))
# print(paste0(color," and feel have cosine similarity rating of ", round(costring(color,"feel" ,tvectors=tk2[,1:ncol(tk2)]),2)))

```

As you can see, my guesses about what "star" means are quite off.

#### Is "eye" more closely related to "sight" or "memory"?

Okay, so I was interested in "eye", too.

```{r}
#| include: true
print(paste0("Eye and sight have cosine similarity rating of ", round(costring("eye","sight" ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recal have cosine similarity rating of ", round(costring("eye",'recal' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recogn have cosine similarity rating of ", round(costring("eye",'recogn' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and rememb have cosine similarity rating of ", round(costring("eye",'rememb' ,tvectors=tk2[,1:ncol(tk2)]),2)))
```

As it turns out, my hypothesis that "eye", when projected onto the semantic space, is more closely related to memory words like "recogn" is not supported. Instead, "eye" is more closely related to sight (and even more so "stare" at .55!).

If anyone has any suggestions on what else I could analyze, feel free to let me know in the comments!

# Conclusion

AFI is my favorite band of all time, and Sing the Sorrow is my favorite album of all time. Combined with my love for statistics, I decided to do a first go-around of analyzing the content of AFI's lyrics across their major album releases. Therefore, I excluded EPs, Deluxe Editions, Vinyls, and so on. I found some interesting stuff.

Both the word frequencies and word cloud (with different weightings) found that "feel" and "love" were the most frequently used word stems (with stop words removed). In addition, the Sing the Sorrow album featured words like "grey" and "heart".

Following this, I found that "eye" was most strongly related to "stare", "recal", and "hair", suggesting that staring eyes may signify a recalling memories. I don't have any guesses for "hair". I also found that [This Celluloid Dream](https://www.youtube.com/watch?v=vVZKbIOJIJw) is most closely related to one of AFI's newest songs [Twisted Tongues](https://www.youtube.com/watch?v=_3ezOSq5QXk).

Finally, I discovered several different ways in which "feel" and "love" were used. In some cases, "feel" meant confinement in an unfamiliar setting, and in other cases, "feel" meant receiving or giving gifts of poisonous pieces of oneself or others. In some cases, "love" meant attraction, and in other cases, "love" meant past bloody battles. Of course these are my own interpretations.

Overall, this was a fun exercise, and I certainly learned a lot! I can hardly wait to attend the 20th Anniversary Sing the Sorrow event! For real. I've been in The Despair Faction fan club for 20 years. This is a real treat.

What were your takeaways from this?

[Caleb J. Picker](https://calebjpicker.quarto.pub/) January 18, 2023
