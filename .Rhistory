library(grid)
# afi_sts_bg<-jpeg::readJPEG("afi-sing-sorrow.jpg")
# Prepare only albums for major release
album_major_release<-c("Answer That and Stay Fashionable",
"Very Proud of Ya",
"Shut Your Mouth and Open Your Eyes",
"Black Sails in the Sunset",
"The Art of Drowning",
"Sing the Sorrow",
"DECEMBERUNDERGROUND",
"Crash Love",
"Burials",
"AFI (The Blood Album)",
"Bodies")
# colours for each album
albumCol <- c("#c9a95a", # ATASF
"#a0480f", # VPYa
"#dc4d33", #SYMAOYE
"#e86722", # Black Sails in the Sunset
"#9f62ad", # The Art of Drowning
"#92141c", # Sing the Sorrow
"#d6dee5", # DU
"#f3c937", # Crash Love
"#7c7c7c", # Burials
"#3b0404", # AFI (The Blood Album)
"#595957") # Bodies
names(albumCol) <- album_major_release
# This ensures bars are stacked in order of release date
topFew$album <- factor(topFew$album, levels = album_major_release)
wordsPlot <- ggplot(topFew) +
geom_bar(aes(x = order,
y = n,
fill = as.factor(album)),
colour = "#141517",
stat = "identity",
show.legend = FALSE) + # removes legend
coord_flip() +
facet_wrap(~ album ,scales="free") +
labs(title = "AFI's Top 10 most frequently used words by album",
caption = "Source: genius.com | by @CalebPicker",
y = "Frequency",
x = "Word",
fill = "Album") +
# annotation_custom(rasterGrob(afi_sts_bg,
#                              width = unit(1,"npc"),
#                              height = unit(1,"npc")),
#                              -Inf,Inf,-Inf,Inf) +
scale_fill_manual(values = albumCol) +
# add categories to axis
scale_x_continuous(
breaks = topFew$order,
labels = topFew$word,
expand = c(0,0)
)                                   +
theme(title = element_text(face = "bold", size = 20),
strip.background=element_rect(fill='#141517'),
strip.text.x = element_text(color="#d51020"), # This impacted teh titles for each facet grid
strip.text.y = element_text(color="#d51020"),
strip.text = element_text(size=12,color="#d51020"),
panel.border = element_rect(colour = "#141517", fill=NA, size=1),
panel.background = element_rect(colour = "#767975", fill = "white"),
panel.grid.major.x = element_line(colour="#d51020",size = 1, linetype = 4),
axis.title = element_text(face = "bold",size = 20, colour = "#d51020"),
axis.ticks.length = unit(5, units = "pt"),
legend.position="none",
# legend.background = element_rect(color='#141517', fill='#141517'),
# legend.title = element_text(color='#d51020'),
# legend.position = "top",
# legend.key.size = unit(12,"pt"),
# legend.box.spacing = unit(3,"pt"),
# legend.text = element_text(size = 12,color='#d51020'),
axis.text.y = element_text(size = 12,face="bold"),
axis.text.x = element_text(size = 12,face="bold"),
plot.background=element_rect(fill='#141517',color='#141517'),
plot.title=element_text(color='#d51020'),
plot.subtitle=element_text(color='#d51020'),
)
#| echo: false
#| include: true
#| cache: false
#| fig-height: 12
#| fig-width: 14
#| fig-cap: "Figure 2: Word frequency by each of AFI's albums"
# plotly::ggplotly(wordsPlot,show.legend=FALSE)
wordsPlot
# ggsave(filename = "/Albums/AFIwordsbyalbum.png", plot = wordsPlot, width = 30, height = 24, units = "cm",
# type = "cairo")
#| echo: false
#| freeze: false
#| include: true
#| cache: true
myCosineSpace2<-multicos(myTerms2,tvectors=tk2) # removed breakdown=TRUE arg, and it worked
suppressWarnings(DT::datatable(round(myCosineSpace2,2),
options=list(scrollX=TRUE),
caption="Table 4: Cosine similarity for relationships between individual words."))
# library(LSAfun)
# coherence(myTerms2,tvectors=tk2)
#| include: true
print(paste0("Eye and sight have cosine similarity rating of ", round(costring("eye","sight" ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recal have cosine similarity rating of ", round(costring("eye",'recal' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recogn have cosine similarity rating of ", round(costring("eye",'recogn' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and rememb have cosine similarity rating of ", round(costring("eye",'rememb' ,tvectors=tk2[,1:ncol(tk2)]),2)))
#| include: true
#| echo: false
#| cache: false
#
# Factor loadings
tk2_round_df<-round(as.data.frame(tk2),2)
DT::datatable(tk2_round_df,
options=list(scrollX=TRUE),
caption="Table 5: Words and their loadings onto factors (hidden themes).")
#| include: true
#| echo: false
#| cache: false
# Top 5 of each factor
top_words<-sapply(tk2_round_df,function(x) head(row.names(tk2_round_df)[order(x,decreasing=TRUE)],5))
top_value<-sapply(tk2_round_df,function(x) head(x[order(x,decreasing=TRUE)],5))
top_themes<-data.frame()
top_themes<-cbind(top_words,top_value)
DT::datatable(top_themes,
options=list(scrollX=TRUE,scrollY=TRUE),
caption="Table 6: Top 5 words loading onto their factors (hidden themes).")
#| include: false
#| echo: false
# This will show the matrix of documents
# factoring of documents, # factors = 40, so there are 40 factors with loadings for each documents
dk2 = miniLSAspace$dk
# round(dk2,2)
#DT::datatable(round(dk2,2),
#              fillContainer = TRUE)
#| include: false
#| echo: false
# The sk matrix of singular values connects tk and dk matrices to reproduce the the original TDM2
# Because the sk matrix only has diagonal values, R stores it as a numeric vector
sk2 = miniLSAspace$sk
# round(sk2,2)
# Words to remove
words_to_rm<-c("ah",
"cajz",
"cap'n",
"calvin",
"de",
"hey",
"ho",
"l.a",
"ohh",
"the",
"uh",
"um",
"whoa",
"whoah",
"ya",
"yeah")
# remove words
allLyricsTokenised<-allLyricsTokenised[!(allLyricsTokenised$word %in% words_to_rm),]
allLyricsTokenised <- allLyrics2 %>%
unnest_tokens(word, line,token="words")
library(tidytext)
allLyricsTokenised <- allLyrics2 %>%
unnest_tokens(word, line,token="words")
# Words to remove
words_to_rm<-c("ah",
"cajz",
"cap'n",
"calvin",
"de",
"hey",
"ho",
"l.a",
"ohh",
"the",
"uh",
"um",
"whoa",
"whoah",
"ya",
"yeah")
# remove words
allLyricsTokenised<-allLyricsTokenised[!(allLyricsTokenised$word %in% words_to_rm),]
#| echo: false
#| include: true
#| output: true
#| freeze: false
#| layout-ncol: 2
#| column: page
#| tbl-cap: "Word frequency across AFI's major album releases."
#| tbl-subcap:
#|    - "Stop words present"
#|    - "Stop words removed"
# library(jsonlite)
# Count each word
allLyricscount<-allLyricsTokenised %>%
count(word, sort = TRUE)
DT::datatable(allLyricscount,
caption = "Table 2: Word frequency across all of AFI's major album releases (with stop-words).",
colnames=c("Word","Frequency")
#             options = list(
# initComplete = JS(
#   "function(settings, json) {",
#   "$(this.api().table().header()).css({'background-color': '#92141c', 'color': '#767975'});",
#   "}")
)
# library(rjson)
library(jsonlite)
# library(rjson)
# Remove stopwords
tidyLyrics <- allLyricsTokenised %>%
anti_join(stop_words)
tidyLyricscount<-tidyLyrics %>%
count(word, sort = TRUE)
tidyLyricscount1<- tidyLyrics %>%
count(song_name,word, sort = TRUE)
DT::datatable(tidyLyricscount,
caption = "Table 3: Word frequency across all of AFI's major album releases (with stop-words removed).",
colnames=c("Word","Frequency"))
#             options = list(
# initComplete = JS(
#   "function(settings, json) {",
#   "$(this.api().table().header()).css({'background-color': '#92141c', 'color': '#767975'});",
#   "}")
#| echo: false
#| freeze: true
#| cache: true
#| include: false
library(LSAfun)
library(lsa)
# Import list of default stop words in English
data(stopwords_en)
# stopwords_en
# Create the TDM from all the Words
tidyLyrics_select<-tidyLyrics %>%
select(album,song_name,word) # song name and word
# DT::datatable(tidyLyrics_select) # for troubleshooting
# split the tidyLyrics into multiple dataframes based on song
AFI_songs_df_split<- split(tidyLyrics_select,tidyLyrics_select$song_name)
# change name of Prelude 12/21 list to remove slash '/' both the name of hte list and within the list
names(AFI_songs_df_split)[89]<-"Prelude 12 21"
AFI_songs_df_split[['Prelude 12 21']][1]<-"Prelude 12 21"
#| echo: false
#| freeze: true
#| include: false
#| cache: true
# # Create function to remove song_name column within each list element
# AFI_songs_df_split<-lapply(AFI_songs_df_split, function(x) x[!(names(x) %in% c("song_name"))])
dir<-"C:/Users/Caleb/OneDrive/Caleb/Personal/Financials/Curriculum Vitae/Blog"
# # Create function to export as txt files
lapply(1:length(AFI_songs_df_split),function(x){
songname<-as.character(names(AFI_songs_df_split[x]))
outfile<-paste0(dir,"/AFIsongs/",songname,".txt")
write.table(AFI_songs_df_split[[x]]["word"],
file = outfile,
row.names=FALSE,
col.names=FALSE, # remove the word "word" from all documents
quote=FALSE)
})
#| freeze: false
#| echo: false
#| include: false
#| cache: true
# Next I will follow a more practical example so I can learn how to interpret the output better, besides a wordcloud
# Also, make sure that all from major albumreleases are included and others are excluded (didn't seem to be the case for some reason)
# Load required code libraries
library(cluster)
library(tm)
library(LSAfun)
library(lsa)
dir = 'C:/Users/caleb/OneDrive/Caleb/Personal/Financials/Curriculum Vitae/Blog'
setwd(dir)
# Create text matrix using tm's DirSource.
# Create corpus in memory
# There's some mistyped code that doesn't work in this paper
# raw_corpus<-VCorpus(source_dir,readerControl-list(language='en'))
#I'll copy and paste from previous example to create a raw corpus
# Establish source directory for all text files. (one .txt file = 1 song)
source_dir<-paste0(dir,"/AFIsongs/")
data(stopwords_en)
# Create TDM (text document matrix) with each txt file equal to one document
TDM<-textmatrix(source_dir,stopwords=c(stopwords_en),stemming=TRUE,
removeNumber=F,minGlobFreq=1.75)
# TDM #  view TDM
TDM_summary<-summary.textmatrix(TDM)
TDM_summary_terms<-t(TDM_summary["vocabulary"])
TDM_summary_docs<-t(TDM_summary["documents"])
# Before running the SVM, create a weighted matrix TDM2.
# TDM2 is term frequency x inverse document frequency (this is a standard method)
# lw = local weighting, more importance to terms that appear more times within a single document
# gw = global weighting, less importance to w9rds that appear in more document (same reasoning as removing stop words)
TDM2 <-lw_tf(TDM)*gw_idf(TDM)
# TDM2
#| include: false
#| echo: false
library(cluster)
library(tm)
library(LSAfun)
library(lsa)
# Run LSA on wegihted matrix TDM2.
# Number of dimensions chosen by default with dimcalc_share()
# lsa transformed TDM2 into three matrices and placed them all into the miniLSAspace object
# tk = term matrix, dk = document matrix, and sk = singular value matrix
miniLSAspace<-lsa(TDM2,dims=dimcalc_share())
# miniLSAspace<-lsa(TDM2,dims=5)
# View matrix by transforming SVD matrix into text matrix
as.textmatrix(miniLSAspace)
# Doesn't work.  Needs other non-working script.
# findFreqTerms(TDM2)
# these are all unrotated "loadings", like PCA or factor analysis
# This command will show the value-weighted matrix of Terms
# factoring of terms, # factors = 40, so there are 40 factors with loadings for each term
tk2<-t(miniLSAspace$sk*t(miniLSAspace$tk))
tk2_numfactors<-ncol(tk2)
#| echo: false
#| include: true
#| cache: false
#| tbl-cap: "Figure 1: Wordcloud of the lyrics to each of AFI's major album releases."
library(wordcloud)
# plot wordcloud using lsa results!!!
Term_count<-apply(TDM2,1,sum)
TCT<-t(Term_count)
myTerms2<-rownames(tk2)
suppressWarnings(wordcloud(myTerms2,TCT,min.freq=3,random.order=FALSE,color=brewer.pal(8,"Dark2")
,scale=c(3.5,.1)))
#| echo: false
#| freeze: true
#| output: false
#| include: false
library(tidyverse)
topFew <- tidyLyrics %>%
group_by(album, word) %>%
mutate(n = row_number()) %>%
ungroup()
# Take only top 10 max for each word by album
topFew <- topFew %>%
group_by(album, word) %>%
summarise(n = max(n))%>%
mutate(total=sum(n)) %>%
top_n(10,n) %>%
ungroup() %>%
arrange(album,n) %>%
# 3. add order column of row numbers
mutate(order=row_number())
topFew <- topFew %>%
mutate(total = sum(n)) %>%
group_by(word) %>%
# top_n(10,abs(n)) %>%
# filter(total>=40) %>%
# 1. remove grouping
ungroup() %>%
# 2. Arrange by
# i. facet group
# ii. bar height
arrange(album,total) %>%
# 3. add order column of row numbers
mutate(order=row_number())
#| echo: false
#| cache: false
# Create a facet for each album and list the most frequented words for that album
# https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets
library(plotly) # package to make plot interactive
library(jpeg)
library(grid)
# afi_sts_bg<-jpeg::readJPEG("afi-sing-sorrow.jpg")
# Prepare only albums for major release
album_major_release<-c("Answer That and Stay Fashionable",
"Very Proud of Ya",
"Shut Your Mouth and Open Your Eyes",
"Black Sails in the Sunset",
"The Art of Drowning",
"Sing the Sorrow",
"DECEMBERUNDERGROUND",
"Crash Love",
"Burials",
"AFI (The Blood Album)",
"Bodies")
# colours for each album
albumCol <- c("#c9a95a", # ATASF
"#a0480f", # VPYa
"#dc4d33", #SYMAOYE
"#e86722", # Black Sails in the Sunset
"#9f62ad", # The Art of Drowning
"#92141c", # Sing the Sorrow
"#d6dee5", # DU
"#f3c937", # Crash Love
"#7c7c7c", # Burials
"#3b0404", # AFI (The Blood Album)
"#595957") # Bodies
names(albumCol) <- album_major_release
# This ensures bars are stacked in order of release date
topFew$album <- factor(topFew$album, levels = album_major_release)
wordsPlot <- ggplot(topFew) +
geom_bar(aes(x = order,
y = n,
fill = as.factor(album)),
colour = "#141517",
stat = "identity",
show.legend = FALSE) + # removes legend
coord_flip() +
facet_wrap(~ album ,scales="free") +
labs(title = "AFI's Top 10 most frequently used words by album",
caption = "Source: genius.com | by @CalebPicker",
y = "Frequency",
x = "Word",
fill = "Album") +
# annotation_custom(rasterGrob(afi_sts_bg,
#                              width = unit(1,"npc"),
#                              height = unit(1,"npc")),
#                              -Inf,Inf,-Inf,Inf) +
scale_fill_manual(values = albumCol) +
# add categories to axis
scale_x_continuous(
breaks = topFew$order,
labels = topFew$word,
expand = c(0,0)
)                                   +
theme(title = element_text(face = "bold", size = 20),
strip.background=element_rect(fill='#141517'),
strip.text.x = element_text(color="#d51020"), # This impacted teh titles for each facet grid
strip.text.y = element_text(color="#d51020"),
strip.text = element_text(size=12,color="#d51020"),
panel.border = element_rect(colour = "#141517", fill=NA, size=1),
panel.background = element_rect(colour = "#767975", fill = "white"),
panel.grid.major.x = element_line(colour="#d51020",size = 1, linetype = 4),
axis.title = element_text(face = "bold",size = 20, colour = "#d51020"),
axis.ticks.length = unit(5, units = "pt"),
legend.position="none",
# legend.background = element_rect(color='#141517', fill='#141517'),
# legend.title = element_text(color='#d51020'),
# legend.position = "top",
# legend.key.size = unit(12,"pt"),
# legend.box.spacing = unit(3,"pt"),
# legend.text = element_text(size = 12,color='#d51020'),
axis.text.y = element_text(size = 12,face="bold"),
axis.text.x = element_text(size = 12,face="bold"),
plot.background=element_rect(fill='#141517',color='#141517'),
plot.title=element_text(color='#d51020'),
plot.subtitle=element_text(color='#d51020'),
)
#| echo: false
#| include: true
#| cache: false
#| fig-height: 12
#| fig-width: 14
#| fig-cap: "Figure 2: Word frequency by each of AFI's albums"
# plotly::ggplotly(wordsPlot,show.legend=FALSE)
wordsPlot
# ggsave(filename = "/Albums/AFIwordsbyalbum.png", plot = wordsPlot, width = 30, height = 24, units = "cm",
# type = "cairo")
#| echo: false
#| freeze: false
#| include: true
#| cache: true
myCosineSpace2<-multicos(myTerms2,tvectors=tk2) # removed breakdown=TRUE arg, and it worked
suppressWarnings(DT::datatable(round(myCosineSpace2,2),
options=list(scrollX=TRUE),
caption="Table 4: Cosine similarity for relationships between individual words."))
# library(LSAfun)
# coherence(myTerms2,tvectors=tk2)
#| include: true
print(paste0("Eye and sight have cosine similarity rating of ", round(costring("eye","sight" ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recal have cosine similarity rating of ", round(costring("eye",'recal' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and recogn have cosine similarity rating of ", round(costring("eye",'recogn' ,tvectors=tk2[,1:ncol(tk2)]),2)))
print(paste0("Eye and rememb have cosine similarity rating of ", round(costring("eye",'rememb' ,tvectors=tk2[,1:ncol(tk2)]),2)))
#| include: true
#| echo: false
#| cache: false
#
# Factor loadings
tk2_round_df<-round(as.data.frame(tk2),2)
DT::datatable(tk2_round_df,
options=list(scrollX=TRUE),
caption="Table 5: Words and their loadings onto factors (hidden themes).")
#| include: true
#| echo: false
#| cache: false
# Top 5 of each factor
top_words<-sapply(tk2_round_df,function(x) head(row.names(tk2_round_df)[order(x,decreasing=TRUE)],5))
top_value<-sapply(tk2_round_df,function(x) head(x[order(x,decreasing=TRUE)],5))
top_themes<-data.frame()
top_themes<-cbind(top_words,top_value)
DT::datatable(top_themes,
options=list(scrollX=TRUE,scrollY=TRUE),
caption="Table 6: Top 5 words loading onto their factors (hidden themes).")
#| include: true
#| echo: false
#| cache: false
#
# Factor loadings
tk2_round_df<-round(as.data.frame(tk2),2)
DT::datatable(tk2_round_df,
options=list(scrollX=TRUE),
caption="Table 5: Words and their loadings onto factors (hidden themes).")
#| include: true
#| echo: false
#| cache: true
#
# Factor loadings
tk2_round_df<-round(as.data.frame(tk2),2)
DT::datatable(tk2_round_df,
options=list(scrollX=TRUE),
caption="Table 5: Words and their loadings onto factors (hidden themes).")
#| include: true
#| echo: false
#| cache: false
tk2_round_df<-round(as.data.frame(tk2),2)
# Top 5 of each factor
top_words<-sapply(tk2_round_df,function(x) head(row.names(tk2_round_df)[order(x,decreasing=TRUE)],5))
top_value<-sapply(tk2_round_df,function(x) head(x[order(x,decreasing=TRUE)],5))
top_themes<-data.frame()
top_themes<-cbind(top_words,top_value)
DT::datatable(top_themes,
options=list(scrollX=TRUE,scrollY=TRUE),
caption="Table 5: Top 5 words loading onto their factors (hidden themes).")
round(as.data.frame(tk2),2)
